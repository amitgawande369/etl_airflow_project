# ETL Pipeline with Apache Airflow and SQLite/PostgreSQL

## ğŸ“Œ Project Overview
This project demonstrates how to build an **ETL (Extract, Transform, Load) pipeline** using **Apache Airflow** to orchestrate data workflows.  
The pipeline extracts data from multiple sources (CSV files, JSON files, and APIs), transforms the data, and loads it into a relational database such as **PostgreSQL** or **SQLite**.

The goal of this project is to showcase data engineering fundamentals, workflow orchestration, and automation using Airflow.

---

## ğŸ—ï¸ Architecture
**Data Sources â†’ Airflow DAG â†’ Transformations â†’ Relational Database**

- **Extract**
  - CSV files
  - JSON files
  - Public APIs
- **Transform**
  - Data cleaning
  - Type conversions
  - Removing duplicates
  - Normalization
- **Load**
  - PostgreSQL or SQLite database

---

## ğŸ› ï¸ Technologies Used
- **Apache Airflow** â€“ Workflow orchestration
- **Python** â€“ ETL logic
- **Pandas** â€“ Data transformation
- **SQLite / PostgreSQL** â€“ Data storage
- **SQLAlchemy** â€“ Database connection
- **Docker (optional)** â€“ Containerized Airflow setup

---

## ğŸ“‚ Project Structure  
etl-airflow-pipeline/  
â”‚  
â”œâ”€â”€ dags/  
â”‚ â””â”€â”€ etl_pipeline_dag.py  
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/
â”‚ â”‚ â”œâ”€â”€ sample.csv
â”‚ â”‚ â””â”€â”€ sample.json
â”‚ â””â”€â”€ processed/
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ extract.py
â”‚ â”œâ”€â”€ transform.py
â”‚ â””â”€â”€ load.py
â”‚
â”œâ”€â”€ database/
â”‚ â””â”€â”€ etl.db
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
